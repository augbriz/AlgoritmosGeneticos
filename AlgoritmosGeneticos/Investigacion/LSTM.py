import numpy as np                
import pandas as pd               
import matplotlib.pyplot as plt   
import warnings
warnings.filterwarnings('ignore')
import datetime
today = datetime.date.today()
print("Fecha actual:", today) 
# importaciÃ³n de los datos 
import yfinance as yf            
# Framework deep learning
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam, RMSprop
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.regularizers import l2

# Preprocesamiento de los datos
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error

# --- ConfiguraciÃ³n de TensorFlow ---
print("TensorFlow version:", tf.__version__)
print("GPU disponible:", tf.config.list_physical_devices('GPU'))

# Configurar reproducibilidad
tf.random.set_seed(42)
np.random.seed(42)

print("âœ… Importaciones completadas exitosamente")
print("ğŸ¯ Preparado para implementar modelo LSTM")

# =============================================================================
# CONSTANTES DEL PROYECTO


# ConfiguraciÃ³n de datos
TICKER = "YPF"
START_DATE = "2008-01-01"
END_DATE = today
NUM_DATOS = 1000  # Ãšltimos 1000 datos (como en anÃ¡lisis caÃ³tico)

# ConfiguraciÃ³n del modelo (valores base - se optimizarÃ¡n con AG)
LOOKBACK_WINDOW = 20      # Ventana temporal: Ãºltimos 20 dÃ­as para predecir
TRAIN_SPLIT = 0.7         # 70% para entrenamiento
VAL_SPLIT = 0.15          # 15% para validaciÃ³n
TEST_SPLIT = 0.15         # 15% para prueba

print(f"ğŸ“Š ConfiguraciÃ³n del proyecto:")
print(f"   â€¢ Ticker: {TICKER}")
print(f"   â€¢ Datos: {NUM_DATOS} puntos histÃ³ricos")
print(f"   â€¢ Ventana temporal: {LOOKBACK_WINDOW} dÃ­as")
print(f"   â€¢ DivisiÃ³n: {TRAIN_SPLIT:.0%} train, {VAL_SPLIT:.0%} val, {TEST_SPLIT:.0%} test")


# CARGA Y VISUALIZACIÃ“N DE DATOS


def cargar_datos_ypf():
    """
    Carga los datos histÃ³ricos de YPF usando la misma configuraciÃ³n
    que en el anÃ¡lisis caÃ³tico para mantener consistencia
    
    Returns:
        pd.Series: Serie temporal con los precios de cierre de YPF
    """
    print("\nğŸ“¥ Descargando datos de YPF...")
    
    # Descargar datos (igual que en YPF.py para mantener consistencia)
    data = yf.download(TICKER, start=START_DATE, end=END_DATE, progress=False)
    
    # Verificar que los datos se descargaron correctamente
    if data.empty or 'Close' not in data.columns:
        raise ValueError(f"No se pudieron obtener datos de cierre para {TICKER}")
    
    # Tomar los Ãºltimos NUM_DATOS puntos (1000, como en anÃ¡lisis caÃ³tico)
    precios_cierre = data['Close'].dropna().tail(NUM_DATOS)
    
    print(f"âœ… Datos cargados exitosamente:")
    print(f"   â€¢ Total de datos: {len(precios_cierre)}")
    print(f"   â€¢ Rango de fechas: {precios_cierre.index[0].date()} a {precios_cierre.index[-1].date()}")
    print(f"   â€¢ Precio mÃ­nimo: ${float(precios_cierre.min()):.2f}")
    print(f"   â€¢ Precio mÃ¡ximo: ${float(precios_cierre.max()):.2f}")
    print(f"   â€¢ Precio promedio: ${float(precios_cierre.mean()):.2f}")
    print(f"   â€¢ Volatilidad (std): ${float(precios_cierre.std()):.2f}")
    
    return precios_cierre

def visualizar_datos(precios):
    """
    Visualiza los datos cargados para verificar que todo estÃ© correcto
    
    Args:
        precios (pd.Series): Serie temporal con los precios de cierre
    """
    print("\nğŸ“Š Creando visualizaciÃ³n de la serie temporal...")
    
    plt.figure(figsize=(14, 8))
    
    # GrÃ¡fico principal
    plt.subplot(2, 1, 1)
    plt.plot(precios.index, precios.values, linewidth=0.8, color='blue', alpha=0.8)
    plt.title(f'Serie Temporal - {TICKER} (Ãšltimos {len(precios)} dÃ­as)', fontsize=16, fontweight='bold')
    plt.xlabel('Fecha', fontsize=12)
    plt.ylabel('Precio de Cierre (USD)', fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.xticks(rotation=45)
    
    # Agregar informaciÃ³n estadÃ­stica en el grÃ¡fico
    stats_text = f'Datos: {len(precios)} puntos\nÃšltimo precio: ${float(precios.iloc[-1]):.2f}\nPromedio: ${float(precios.mean()):.2f}'
    plt.text(0.02, 0.98, stats_text, 
             transform=plt.gca().transAxes, fontsize=10, 
             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
    
    # GrÃ¡fico de distribuciÃ³n de precios
    plt.subplot(2, 1, 2)
    plt.hist(precios.values, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
    plt.title('DistribuciÃ³n de Precios', fontsize=14)
    plt.xlabel('Precio (USD)', fontsize=12)
    plt.ylabel('Frecuencia', fontsize=12)
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    


def analizar_datos_basico(precios):
    """
    AnÃ¡lisis bÃ¡sico de los datos para entender mejor la serie temporal
    
    Args:
        precios (pd.Series): Serie temporal con los precios de cierre
    """
    print("\nğŸ” AnÃ¡lisis bÃ¡sico de la serie temporal:")
    
    # EstadÃ­sticas descriptivas
    print(f"\nğŸ“ˆ EstadÃ­sticas descriptivas:")
    print(f"   â€¢ Cuenta: {len(precios)}")
    print(f"   â€¢ Media: ${float(precios.mean()):.2f}")
    print(f"   â€¢ Mediana: ${float(precios.median()):.2f}")
    print(f"   â€¢ DesviaciÃ³n estÃ¡ndar: ${float(precios.std()):.2f}")
    print(f"   â€¢ MÃ­nimo: ${float(precios.min()):.2f}")
    print(f"   â€¢ MÃ¡ximo: ${float(precios.max()):.2f}")
    
    # AnÃ¡lisis de tendencia bÃ¡sico
    precio_inicial = float(precios.iloc[0])
    precio_final = float(precios.iloc[-1])
    variacion_total = ((precio_final - precio_inicial) / precio_inicial) * 100
    
    print(f"\nğŸ“Š AnÃ¡lisis de tendencia:")
    print(f"   â€¢ Precio inicial: ${precio_inicial:.2f}")
    print(f"   â€¢ Precio final: ${precio_final:.2f}")
    print(f"   â€¢ VariaciÃ³n total: {variacion_total:+.2f}%")
    
    # AnÃ¡lisis de volatilidad bÃ¡sico
    returns = precios.pct_change().dropna()
    volatilidad_diaria = float(returns.std())
    volatilidad_anualizada = volatilidad_diaria * np.sqrt(252)  # 252 dÃ­as de trading por aÃ±o
    
    print(f"\nâš¡ AnÃ¡lisis de volatilidad:")
    print(f"   â€¢ Volatilidad diaria: {volatilidad_diaria:.4f}")
    print(f"   â€¢ Volatilidad anualizada: {volatilidad_anualizada:.4f} ({volatilidad_anualizada*100:.2f}%)")
    
    return {
        'estadisticas': precios.describe(),
        'variacion_total': variacion_total,
        'volatilidad_anualizada': volatilidad_anualizada,
        'returns': returns
    }

# Ejecutar la carga y anÃ¡lisis de datos
print("\n" + "="*70)
print("ğŸš€ FASE 1: CARGA Y EXPLORACIÃ“N DE DATOS")
print("="*70)

# Cargar datos
datos_ypf = cargar_datos_ypf()

# Visualizar datos
visualizar_datos(datos_ypf)

# AnÃ¡lisis bÃ¡sico
analisis_basico = analizar_datos_basico(datos_ypf)

# =============================================================================
# PASO 3: PREPROCESAMIENTO PARA LSTM
# =============================================================================

def normalizar_datos(serie):
    """
    Normaliza los datos a escala 0-1 usando MinMaxScaler
    
    Â¿Por quÃ© normalizar?
    - Las LSTM funcionan mejor con datos en escala pequeÃ±a (0-1)
    - Evita que valores grandes dominen el entrenamiento
    - Acelera la convergencia del modelo
    
    Args:
        serie (pd.Series): Serie temporal de precios
    
    Returns:
        tuple: (datos_normalizados, scaler_objeto)
    """
    print("\nğŸ”„ Normalizando datos a escala 0-1...")
    
    # Crear el escalador MinMax (0-1)
    scaler = MinMaxScaler(feature_range=(0, 1))
    
    # Convertir serie a array numpy y reshape para sklearn
    datos_array = serie.values.reshape(-1, 1)
    
    # Ajustar el scaler y transformar los datos
    datos_normalizados = scaler.fit_transform(datos_array)
    
    print(f"âœ… NormalizaciÃ³n completada:")
    print(f"   â€¢ Valor original mÃ­nimo: ${float(serie.min()):.2f}")
    print(f"   â€¢ Valor original mÃ¡ximo: ${float(serie.max()):.2f}")
    print(f"   â€¢ Valor normalizado mÃ­nimo: {datos_normalizados.min():.4f}")
    print(f"   â€¢ Valor normalizado mÃ¡ximo: {datos_normalizados.max():.4f}")
    
    return datos_normalizados.flatten(), scaler

def crear_secuencias_temporales(datos, lookback_window, prediction_horizon=1):
    """
    Crea secuencias temporales para entrenar la LSTM
    
    Â¿QuÃ© hace?
    - Convierte la serie temporal en ventanas deslizantes
    - Cada ventana tiene 'lookback_window' dÃ­as de historia
    - El objetivo es predecir el siguiente dÃ­a (o dÃ­as)
    
    Ejemplo: Si lookback_window=3
    [1,2,3,4,5,6,7] se convierte en:
    X=[[1,2,3], [2,3,4], [3,4,5], [4,5,6]]
    y=[4, 5, 6, 7]
    
    Args:
        datos (array): Datos normalizados
        lookback_window (int): Ventana temporal (dÃ­as pasados)
        prediction_horizon (int): DÃ­as a predecir (normalmente 1)
    
    Returns:
        tuple: (X, y) arrays para entrenamiento
    """
    print(f"\nğŸ—ï¸ Creando secuencias temporales...")
    print(f"   â€¢ Ventana temporal: {lookback_window} dÃ­as")
    print(f"   â€¢ Horizonte de predicciÃ³n: {prediction_horizon} dÃ­a(s)")
    
    X, y = [], []
    
    # Crear ventanas deslizantes
    for i in range(lookback_window, len(datos) - prediction_horizon + 1):
        # Ventana de entrada (Ãºltimos 'lookback_window' dÃ­as)
        X.append(datos[i-lookback_window:i])
        # Objetivo (siguiente dÃ­a o dÃ­as)
        if prediction_horizon == 1:
            y.append(datos[i])  # Solo el siguiente dÃ­a
        else:
            y.append(datos[i:i+prediction_horizon])  # MÃºltiples dÃ­as
    
    X, y = np.array(X), np.array(y)
    
    print(f"âœ… Secuencias creadas:")
    print(f"   â€¢ Total de secuencias: {len(X)}")
    print(f"   â€¢ Forma de X (entrada): {X.shape}")
    print(f"   â€¢ Forma de y (objetivo): {y.shape}")
    
    # Reshape X para LSTM: (samples, timesteps, features)
    X = X.reshape((X.shape[0], X.shape[1], 1))
    print(f"   â€¢ X reshape para LSTM: {X.shape}")
    
    return X, y

def dividir_datos(X, y, train_split=0.7, val_split=0.15, test_split=0.15):
    """
    Divide los datos en entrenamiento, validaciÃ³n y prueba
    
    Â¡IMPORTANTE! Para series temporales NO usamos divisiÃ³n aleatoria
    Mantenemos el orden temporal:
    - Train: datos mÃ¡s antiguos
    - Validation: datos intermedios  
    - Test: datos mÃ¡s recientes
    
    Args:
        X, y: Arrays de secuencias
        train_split, val_split, test_split: Proporciones de divisiÃ³n
    
    Returns:
        tuple: (X_train, X_val, X_test, y_train, y_val, y_test)
    """
    print(f"\nâœ‚ï¸ Dividiendo datos temporalmente...")
    
    # Validar que las proporciones sumen 1
    if abs(train_split + val_split + test_split - 1.0) > 0.001:
        raise ValueError("Las proporciones deben sumar 1.0")
    
    n_samples = len(X)
    
    # Calcular Ã­ndices de corte (manteniendo orden temporal)
    train_end = int(n_samples * train_split)
    val_end = int(n_samples * (train_split + val_split))
    
    # Dividir manteniendo orden temporal
    X_train = X[:train_end]
    X_val = X[train_end:val_end]
    X_test = X[val_end:]
    
    y_train = y[:train_end]
    y_val = y[train_end:val_end]
    y_test = y[val_end:]
    
    print(f"âœ… DivisiÃ³n completada:")
    print(f"   â€¢ Entrenamiento: {len(X_train)} secuencias ({len(X_train)/n_samples:.1%})")
    print(f"   â€¢ ValidaciÃ³n: {len(X_val)} secuencias ({len(X_val)/n_samples:.1%})")
    print(f"   â€¢ Prueba: {len(X_test)} secuencias ({len(X_test)/n_samples:.1%})")
    
    return X_train, X_val, X_test, y_train, y_val, y_test

def visualizar_normalizacion(datos_originales, datos_normalizados):
    """
    Visualiza el efecto de la normalizaciÃ³n
    """
    print("\nğŸ“Š Creando visualizaciÃ³n de la normalizaciÃ³n...")
    
    plt.figure(figsize=(15, 6))
    
    # Datos originales
    plt.subplot(1, 2, 1)
    plt.plot(datos_originales.values, color='blue', alpha=0.8)
    plt.title('Datos Originales', fontsize=14, fontweight='bold')
    plt.ylabel('Precio (USD)', fontsize=12)
    plt.xlabel('Tiempo (dÃ­as)', fontsize=12)
    plt.grid(True, alpha=0.3)
    
    # Datos normalizados
    plt.subplot(1, 2, 2)
    plt.plot(datos_normalizados, color='red', alpha=0.8)
    plt.title('Datos Normalizados (0-1)', fontsize=14, fontweight='bold')
    plt.ylabel('Valor Normalizado', fontsize=12)
    plt.xlabel('Tiempo (dÃ­as)', fontsize=12)
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    print("âœ… VisualizaciÃ³n de normalizaciÃ³n completada")

# Ejecutar preprocesamiento
print("\n" + "="*70)
print("ğŸš€ FASE 2: PREPROCESAMIENTO PARA LSTM")
print("="*70)

# Paso 1: Normalizar datos
datos_normalizados, scaler = normalizar_datos(datos_ypf)

# Paso 2: Visualizar normalizaciÃ³n
visualizar_normalizacion(datos_ypf, datos_normalizados)

# Paso 3: Crear secuencias temporales
X, y = crear_secuencias_temporales(datos_normalizados, LOOKBACK_WINDOW)

# Paso 4: Dividir datos
X_train, X_val, X_test, y_train, y_val, y_test = dividir_datos(
    X, y, TRAIN_SPLIT, VAL_SPLIT, TEST_SPLIT
)

print(f"\nğŸ¯ Preprocesamiento completado exitosamente!")
print(f"âœ… Datos listos para entrenar la LSTM")

# =============================================================================
# FASE 3: CONSTRUCCIÃ“N DEL MODELO LSTM
# =============================================================================

def crear_modelo_lstm(input_shape, 
                     lstm_units=50, 
                     num_layers=2, 
                     dropout_rate=0.2,
                     learning_rate=0.001):
    """
    Crea el modelo LSTM con arquitectura optimizada para series financieras
    
    Args:
        input_shape: Forma de entrada (timesteps, features)
        lstm_units: Neuronas por capa LSTM
        num_layers: NÃºmero de capas LSTM
        dropout_rate: Tasa de dropout para regularizaciÃ³n
        learning_rate: Tasa de aprendizaje del optimizador
    
    Returns:
        Modelo LSTM compilado y listo para entrenar
    """
    print(f"\nğŸ§  Construyendo modelo LSTM...")
    print(f"   â€¢ Input shape: {input_shape}")
    print(f"   â€¢ LSTM units: {lstm_units}")
    print(f"   â€¢ Num layers: {num_layers}")
    print(f"   â€¢ Dropout rate: {dropout_rate}")
    print(f"   â€¢ Learning rate: {learning_rate}")
    
    model = Sequential()
    
    # Primera capa LSTM
    model.add(LSTM(
        units=lstm_units,
        return_sequences=True if num_layers > 1 else False,
        input_shape=input_shape,
        name='LSTM_1'
    ))
    model.add(Dropout(dropout_rate, name='Dropout_1'))
    
    # Capas LSTM adicionales
    for i in range(2, num_layers + 1):
        return_seq = True if i < num_layers else False
        model.add(LSTM(
            units=lstm_units,
            return_sequences=return_seq,
            name=f'LSTM_{i}'
        ))
        model.add(Dropout(dropout_rate, name=f'Dropout_{i}'))
    
    # Capa de salida
    model.add(Dense(1, activation='linear', name='Output'))
    
    # Compilar modelo
    model.compile(
        optimizer=Adam(learning_rate=learning_rate),
        loss='mse',
        metrics=['mae']
    )
    
    print(f"âœ… Modelo construido exitosamente")
    return model

def configurar_callbacks():
    """
    Configura callbacks para controlar el entrenamiento
    """
    callbacks = [
        EarlyStopping(
            monitor='val_loss',
            patience=15,
            restore_best_weights=True,
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=5,
            min_lr=1e-7,
            verbose=1
        )
    ]
    return callbacks

def visualizar_arquitectura_modelo(modelo):
    """
    Crea visualizaciones intuitivas de la arquitectura LSTM
    
    Args:
        modelo: Modelo LSTM compilado
    """
    print("\nğŸ¨ Creando visualizaciones de la arquitectura...")
    
    # 1. Resumen textual del modelo
    print("\nğŸ“‹ Resumen de la arquitectura:")
    modelo.summary()
    
    # 2. VisualizaciÃ³n grÃ¡fica con plot_model
    try:
        from tensorflow.keras.utils import plot_model
        import os
        
        print("\nğŸ–¼ï¸ Intentando crear diagrama de arquitectura...")
        
        # Crear el grÃ¡fico de arquitectura
        plot_model(
            modelo, 
            to_file='lstm_architecture.png',
            show_shapes=True,
            show_layer_names=True,
            rankdir='TB',  # Top to Bottom
            expand_nested=True,
            dpi=150
        )
        
        if os.path.exists('lstm_architecture.png'):
            print("âœ… Diagrama de arquitectura guardado como 'lstm_architecture.png'")
            print("ğŸ” Puedes abrir el archivo para ver la arquitectura grÃ¡fica")
        else:
            print("âŒ No se pudo crear el archivo de diagrama")
        
    except ImportError as e:
        print("âš ï¸ Para visualizaciÃ³n grÃ¡fica automÃ¡tica, instala:")
        print("   pip install pydot")
        print("   Y descarga graphviz desde: https://graphviz.gitlab.io/download/")
        print(f"   Error: {str(e)}")
    except Exception as e:
        print(f"âŒ Error al crear diagrama: {str(e)}")
        print("ğŸ”„ Continuando con visualizaciÃ³n manual...")
    
    # 3. VisualizaciÃ³n manual de la arquitectura
    visualizar_arquitectura_manual(modelo)

def visualizar_arquitectura_manual(modelo):
    """
    Crea una visualizaciÃ³n manual e intuitiva de la arquitectura LSTM
    """
    print("\nğŸ§  Arquitectura LSTM visualizada:")
    print("="*60)
    
    fig, ax = plt.subplots(figsize=(14, 10))
    
    # Configurar el grÃ¡fico
    ax.set_xlim(0, 10)
    ax.set_ylim(0, 8)
    ax.axis('off')
    
    # TÃ­tulo
    ax.text(5, 7.5, 'Arquitectura LSTM para PredicciÃ³n de YPF', 
            fontsize=16, fontweight='bold', ha='center')
    
    # 1. Datos de entrada
    entrada = plt.Rectangle((0.5, 6), 2, 0.8, fill=True, 
                           facecolor='lightblue', edgecolor='black', linewidth=2)
    ax.add_patch(entrada)
    ax.text(1.5, 6.4, 'ENTRADA\n(20 dÃ­as, 1 feature)', 
            ha='center', va='center', fontsize=10, fontweight='bold')
    
    # 2. Primera capa LSTM
    lstm1 = plt.Rectangle((0.5, 4.5), 2, 1, fill=True, 
                         facecolor='lightgreen', edgecolor='black', linewidth=2)
    ax.add_patch(lstm1)
    ax.text(1.5, 5, 'LSTM Layer 1\n(50 unidades)\nMemoria temporal', 
            ha='center', va='center', fontsize=9, fontweight='bold')
    
    # 3. Dropout 1
    dropout1 = plt.Rectangle((3, 4.5), 1.5, 1, fill=True, 
                            facecolor='orange', edgecolor='black', linewidth=2)
    ax.add_patch(dropout1)
    ax.text(3.75, 5, 'Dropout\n(20%)\nRegularizaciÃ³n', 
            ha='center', va='center', fontsize=9, fontweight='bold')
    
    # 4. Segunda capa LSTM
    lstm2 = plt.Rectangle((0.5, 3), 2, 1, fill=True, 
                         facecolor='lightgreen', edgecolor='black', linewidth=2)
    ax.add_patch(lstm2)
    ax.text(1.5, 3.5, 'LSTM Layer 2\n(50 unidades)\nPatrones complejos', 
            ha='center', va='center', fontsize=9, fontweight='bold')
    
    # 5. Dropout 2
    dropout2 = plt.Rectangle((3, 3), 1.5, 1, fill=True, 
                            facecolor='orange', edgecolor='black', linewidth=2)
    ax.add_patch(dropout2)
    ax.text(3.75, 3.5, 'Dropout\n(20%)\nRegularizaciÃ³n', 
            ha='center', va='center', fontsize=9, fontweight='bold')
    
    # 6. Capa densa
    dense = plt.Rectangle((0.5, 1.5), 2, 1, fill=True, 
                         facecolor='lightcoral', edgecolor='black', linewidth=2)
    ax.add_patch(dense)
    ax.text(1.5, 2, 'Dense Layer\n(1 neurona)\nPredicciÃ³n final', 
            ha='center', va='center', fontsize=9, fontweight='bold')
    
    # 7. Salida
    salida = plt.Rectangle((0.5, 0.2), 2, 0.8, fill=True, 
                          facecolor='yellow', edgecolor='black', linewidth=2)
    ax.add_patch(salida)
    ax.text(1.5, 0.6, 'SALIDA\nPrecio predicho\n(normalizado)', 
            ha='center', va='center', fontsize=10, fontweight='bold')
    
    # Flechas de flujo
    arrow_props = dict(arrowstyle='->', lw=2, color='black')
    ax.annotate('', xy=(1.5, 4.5), xytext=(1.5, 5.8), arrowprops=arrow_props)
    ax.annotate('', xy=(1.5, 3), xytext=(1.5, 4.5), arrowprops=arrow_props)
    ax.annotate('', xy=(1.5, 1.5), xytext=(1.5, 3), arrowprops=arrow_props)
    ax.annotate('', xy=(1.5, 0.2), xytext=(1.5, 1.5), arrowprops=arrow_props)
    
    # Explicaciones laterales
    ax.text(6, 6, 'ğŸ“Š FLUJO DE DATOS', fontsize=12, fontweight='bold')
    ax.text(6, 5.5, '1. Entrada: 20 dÃ­as de precios normalizados', fontsize=10)
    ax.text(6, 5.2, '2. LSTM 1: Aprende patrones temporales bÃ¡sicos', fontsize=10)
    ax.text(6, 4.9, '3. Dropout: Previene overfitting', fontsize=10)
    ax.text(6, 4.6, '4. LSTM 2: Aprende patrones mÃ¡s complejos', fontsize=10)
    ax.text(6, 4.3, '5. Dropout: MÃ¡s regularizaciÃ³n', fontsize=10)
    ax.text(6, 4.0, '6. Dense: Convierte a predicciÃ³n numÃ©rica', fontsize=10)
    ax.text(6, 3.7, '7. Salida: Precio del dÃ­a siguiente', fontsize=10)
    
    ax.text(6, 3, 'ğŸ§  CONCEPTOS CLAVE', fontsize=12, fontweight='bold')
    ax.text(6, 2.5, 'â€¢ LSTM: Memoria de largo y corto plazo', fontsize=10)
    ax.text(6, 2.2, 'â€¢ Dropout: "Apaga" neuronas aleatoriamente', fontsize=10)
    ax.text(6, 1.9, 'â€¢ Secuencial: InformaciÃ³n fluye hacia adelante', fontsize=10)
    ax.text(6, 1.6, 'â€¢ 50 unidades: Balance capacidad/eficiencia', fontsize=10)
    
    plt.tight_layout()
    plt.show()

def mostrar_parametros_modelo(modelo):
    """
    Muestra informaciÃ³n detallada sobre los parÃ¡metros del modelo
    """
    print("\nğŸ“Š AnÃ¡lisis de parÃ¡metros del modelo:")
    print("="*50)
    
    total_params = modelo.count_params()
    trainable_params = sum([tf.keras.backend.count_params(w) for w in modelo.trainable_weights])
    
    print(f"ğŸ”¢ Total de parÃ¡metros: {total_params:,}")
    print(f"ğŸ¯ ParÃ¡metros entrenables: {trainable_params:,}")
    print(f"ğŸ”’ ParÃ¡metros no entrenables: {total_params - trainable_params:,}")
    
    # Desglose por capa
    print(f"\nğŸ“‹ Desglose por capas:")
    for i, layer in enumerate(modelo.layers):
        layer_type = type(layer).__name__
        params = layer.count_params()
        trainable = sum([tf.keras.backend.count_params(w) for w in layer.trainable_weights])
        non_trainable = params - trainable
        
        print(f"   â€¢ Capa {i+1} ({layer_type}): {params:,} parÃ¡metros")
        print(f"     - Entrenables: {trainable:,}")
        print(f"     - No entrenables: {non_trainable:,}")
    
    print("\nâœ… AnÃ¡lisis de parÃ¡metros completado")


# =============================================================================
# FASE 4: ENTRENAMIENTO DEL MODELO LSTM
# =============================================================================

def entrenar_modelo(modelo, X_train, y_train, X_val, y_val, 
                   epochs=100, batch_size=32, callbacks=None):
    """
    Entrena el modelo LSTM con los datos proporcionados
    
    Args:
        modelo: Modelo LSTM compilado
        X_train, y_train: Datos de entrenamiento
        X_val, y_val: Datos de validaciÃ³n
        epochs: NÃºmero de Ã©pocas para entrenar
        batch_size: TamaÃ±o del batch
        callbacks: Lista de callbacks para el entrenamiento
    
    Returns:
        Historial del entrenamiento (historial de pÃ©rdidas y mÃ©tricas)
    """
    print("\nğŸš€ Iniciando entrenamiento del modelo LSTM...")
    
    # Entrenar modelo
    historial = modelo.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=callbacks,
        verbose=2
    )
    
    print("âœ… Entrenamiento completado")
    return historial

def graficar_historial(historial):
    """
    Grafica el historial de entrenamiento (pÃ©rdida y mÃ©trica)
    
    Args:
        historial: Historial del entrenamiento (output de model.fit)
    """
    print("\nğŸ“ˆ Graficando historial de entrenamiento...")
    
    try:
        # Verificar que el historial existe y tiene datos
        if not historial or not hasattr(historial, 'history'):
            print("âŒ Error: Historial vacÃ­o o invÃ¡lido")
            return
        
        # Verificar que las claves existen
        required_keys = ['loss', 'val_loss', 'mae', 'val_mae']
        missing_keys = [key for key in required_keys if key not in historial.history]
        
        if missing_keys:
            print(f"âŒ Error: Faltan claves en historial: {missing_keys}")
            print(f"   Claves disponibles: {list(historial.history.keys())}")
            return
        
        print("   âœ“ Historial vÃ¡lido, creando grÃ¡ficos...")
        
        # Configurar matplotlib para evitar problemas en Windows
        plt.ioff()  # Desactivar modo interactivo
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
        
        # PÃ©rdida
        epochs = range(1, len(historial.history['loss']) + 1)
        ax1.plot(epochs, historial.history['loss'], label='PÃ©rdida (train)', linewidth=2, color='blue')
        ax1.plot(epochs, historial.history['val_loss'], label='PÃ©rdida (val)', linewidth=2, color='red')
        ax1.set_title('Historial de PÃ©rdida', fontsize=14, fontweight='bold')
        ax1.set_xlabel('Ã‰poca')
        ax1.set_ylabel('PÃ©rdida (MSE)')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # MÃ©trica (MAE)
        ax2.plot(epochs, historial.history['mae'], label='MAE (train)', linewidth=2, color='blue')
        ax2.plot(epochs, historial.history['val_mae'], label='MAE (val)', linewidth=2, color='red')
        ax2.set_title('Historial de MÃ©trica (MAE)', fontsize=14, fontweight='bold')
        ax2.set_xlabel('Ã‰poca')
        ax2.set_ylabel('MAE')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        plt.close(fig)  # Cerrar la figura explÃ­citamente
        
        print("   âœ… GrÃ¡ficas de historial completadas")
        
        # Mostrar resumen del entrenamiento
        final_train_loss = float(historial.history['loss'][-1])
        final_val_loss = float(historial.history['val_loss'][-1])
        final_train_mae = float(historial.history['mae'][-1])
        final_val_mae = float(historial.history['val_mae'][-1])
        
        print(f"\nğŸ“Š Resumen del entrenamiento:")
        print(f"   â€¢ Ã‰pocas completadas: {len(historial.history['loss'])}")
        print(f"   â€¢ Loss final (train): {final_train_loss:.4f}")
        print(f"   â€¢ Loss final (val): {final_val_loss:.4f}")
        print(f"   â€¢ MAE final (train): {final_train_mae:.4f}")
        print(f"   â€¢ MAE final (val): {final_val_mae:.4f}")
        
        # Verificar overfitting
        if final_val_loss > final_train_loss * 1.5:
            print("   âš ï¸ Posible overfitting detectado (val_loss >> train_loss)")
        else:
            print("   âœ… Modelo parece estar bien generalizado")
            
    except Exception as e:
        print(f"âŒ Error al crear grÃ¡ficos: {str(e)}")
        print("   ğŸ”„ Continuando con el siguiente paso...")
        # Mostrar datos bÃ¡sicos sin grÃ¡fico
        try:
            if historial and hasattr(historial, 'history'):
                epochs = len(historial.history.get('loss', []))
                print(f"   ğŸ“Š Entrenamiento completado: {epochs} Ã©pocas")
        except:
            pass

# Ejecutar entrenamiento
print("\n" + "="*70)
print("ğŸš€ FASE 3: CONSTRUCCIÃ“N Y ENTRENAMIENTO DEL MODELO LSTM")
print("="*70)

# Crear modelo
modelo_lstm = crear_modelo_lstm(input_shape=(LOOKBACK_WINDOW, 1))

# Configurar callbacks
callbacks = configurar_callbacks()

# Entrenar modelo
historial_modelo = entrenar_modelo(
    modelo_lstm, X_train, y_train, X_val, y_val, 
    epochs=100, batch_size=32, callbacks=callbacks
)

# Graficar historial de entrenamiento
print("\nğŸ” Verificando historial de entrenamiento...")
try:
    if 'historial_modelo' in locals() and historial_modelo is not None:
        print("   âœ… Historial encontrado, procediendo a graficar...")
        graficar_historial(historial_modelo)
    else:
        print("   âŒ No se encontrÃ³ historial de entrenamiento")
        print("   ğŸ”„ Continuando con el siguiente paso...")
except Exception as e:
    print(f"   âŒ Error en visualizaciÃ³n de historial: {str(e)}")
    print("   ğŸ”„ Continuando con el siguiente paso...")

# Visualizar arquitectura del modelo
visualizar_arquitectura_modelo(modelo_lstm)

# Mostrar parÃ¡metros del modelo
mostrar_parametros_modelo(modelo_lstm)

# =============================================================================
# FASE 5: EVALUACIÃ“N Y PRUEBA DEL MODELO LSTM
# =============================================================================

def evaluar_modelo(modelo, X_test, y_test):
    """
    EvalÃºa el modelo LSTM con los datos de prueba
    
    Args:
        modelo: Modelo LSTM entrenado
        X_test, y_test: Datos de prueba
    
    Returns:
        dict: PÃ©rdida y mÃ©tricas del modelo en los datos de prueba
    """
    print("\nğŸ” Evaluando modelo con datos de prueba...")
    
    # Evaluar modelo
    resultados = modelo.evaluate(X_test, y_test, verbose=0)
    
    # Crear diccionario de resultados
    metrica_resultados = dict(zip(modelo.metrics_names, resultados))
    
    print(f"âœ… EvaluaciÃ³n completada")
    for nombre, valor in metrica_resultados.items():
        print(f"   â€¢ {nombre}: {valor:.4f}")
    
    return metrica_resultados

def graficar_predicciones(modelo, X, y_real, scaler, titulo='Predicciones del Modelo'):
    """
    Grafica las predicciones del modelo LSTM contra los valores reales
    
    Args:
        modelo: Modelo LSTM entrenado
        X: Datos de entrada (X_test o similar)
        y_real: Valores reales (y_test o similar)
        scaler: Objeto scaler para deshacer la normalizaciÃ³n
        titulo: TÃ­tulo del grÃ¡fico
    """
    print(f"\nğŸ“Š Graficando {titulo}...")
    
    # Hacer predicciones
    y_pred = modelo.predict(X)
    
    # Invertir la normalizaciÃ³n
    y_real_invertido = scaler.inverse_transform(y_real.reshape(-1, 1))
    y_pred_invertido = scaler.inverse_transform(y_pred)
    
    # Graficar
    plt.figure(figsize=(14, 8))
    plt.plot(y_real_invertido, label='Real', linewidth=2, color='blue')
    plt.plot(y_pred_invertido, label='PredicciÃ³n', linewidth=2, color='red')
    plt.title(titulo, fontsize=16, fontweight='bold')
    plt.xlabel('DÃ­as')
    plt.ylabel('Precio (USD)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.show()
    
    print("âœ… GrÃ¡fica de predicciones completada")

# Ejecutar evaluaciÃ³n y prueba
print("\n" + "="*70)
print("ğŸš€ FASE 4: EVALUACIÃ“N Y PRUEBA DEL MODELO LSTM")
print("="*70)

# Evaluar modelo
resultados_evaluacion = evaluar_modelo(modelo_lstm, X_test, y_test)

# Graficar predicciones
graficar_predicciones(modelo_lstm, X_test, y_test, scaler, titulo='Predicciones en Datos de Prueba')

# =============================================================================
# FASE 6: AJUSTE FINO Y OPTIMIZACIÃ“N DEL MODELO LSTM
# =============================================================================

def ajustar_modelo(modelo, X_train, y_train, X_val, y_val, 
                  epochs=50, batch_size=16, callbacks=None):
    """
    Ajusta el modelo LSTM con nuevos parÃ¡metros de entrenamiento
    
    Args:
        modelo: Modelo LSTM entrenado
        X_train, y_train: Datos de entrenamiento
        X_val, y_val: Datos de validaciÃ³n
        epochs: NÃºmero de Ã©pocas para entrenar
        batch_size: TamaÃ±o del batch
        callbacks: Lista de callbacks para el entrenamiento
    
    Returns:
        Historial del ajuste (historial de pÃ©rdidas y mÃ©tricas)
    """
    print("\nğŸ”§ Ajustando modelo LSTM...")
    
    # Ajustar modelo
    historial_ajuste = modelo.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=callbacks,
        verbose=2
    )
    
    print("âœ… Ajuste completado")
    return historial_ajuste

# Ejecutar ajuste fino (opcional)
# historial_ajuste = ajustar_modelo(
#     modelo_lstm, X_train, y_train, X_val, y_val, 
#     epochs=50, batch_size=16, callbacks=callbacks
# )

print("\nğŸ‰ Proceso completado exitosamente!")
print("ğŸ”‘ Modelo LSTM listo para hacer predicciones")

# =============================================================================
# VISUALIZACIONES ADICIONALES Y ANÃLISIS DETALLADO
# =============================================================================

print("\n" + "="*70)
print("ğŸ¨ VISUALIZACIONES ADICIONALES Y ANÃLISIS DETALLADO")
print("="*70)

def crear_visualizacion_entrenamiento():
    """
    Prepara funciÃ³n para visualizar el proceso de entrenamiento
    """
    def plot_training_history(history):
        """
        Visualiza las mÃ©tricas durante el entrenamiento
        """
        plt.figure(figsize=(15, 5))
        
        # Loss
        plt.subplot(1, 3, 1)
        plt.plot(history.history['loss'], label='Train Loss', color='blue')
        plt.plot(history.history['val_loss'], label='Validation Loss', color='red')
        plt.title('PÃ©rdida durante entrenamiento')
        plt.xlabel('Epoch')
        plt.ylabel('MSE Loss')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # MAE
        plt.subplot(1, 3, 2)
        plt.plot(history.history['mae'], label='Train MAE', color='blue')
        plt.plot(history.history['val_mae'], label='Validation MAE', color='red')
        plt.title('Error Absoluto Medio')
        plt.xlabel('Epoch')
        plt.ylabel('MAE')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # Learning Rate (si estÃ¡ disponible)
        plt.subplot(1, 3, 3)
        if 'lr' in history.history:
            plt.plot(history.history['lr'], label='Learning Rate', color='green')
            plt.title('Tasa de Aprendizaje')
            plt.xlabel('Epoch')
            plt.ylabel('Learning Rate')
            plt.yscale('log')
            plt.legend()
            plt.grid(True, alpha=0.3)
        else:
            plt.text(0.5, 0.5, 'Learning Rate\nno disponible', 
                    ha='center', va='center', transform=plt.gca().transAxes)
        
        plt.tight_layout()
        plt.show()
    
    return plot_training_history

# Crear funciÃ³n de visualizaciÃ³n de entrenamiento
plot_training_history = crear_visualizacion_entrenamiento()

# Visualizar historial de entrenamiento (si existe)
if 'historial_modelo' in locals():
    print("\nğŸ“ˆ Visualizando historial de entrenamiento...")
    plot_training_history(historial_modelo)

# Visualizaciones detalladas de predicciones
def visualizar_predicciones_detalladas(modelo, X_test, y_test, scaler, n_dias=100):
    """
    Crea visualizaciones detalladas de las predicciones del modelo
    
    Args:
        modelo: Modelo LSTM entrenado
        X_test, y_test: Datos de prueba
        scaler: Objeto MinMaxScaler para desnormalizar
        n_dias: NÃºmero de dÃ­as a mostrar en el grÃ¡fico detallado
    """
    print(f"\nğŸ¨ Creando visualizaciones detalladas de predicciones...")
    
    # Hacer predicciones
    y_pred = modelo.predict(X_test, verbose=0)
    
    # Desnormalizar
    y_real_original = scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()
    y_pred_original = scaler.inverse_transform(y_pred).flatten()
    
    # Calcular mÃ©tricas
    mae = mean_absolute_error(y_real_original, y_pred_original)
    mse = mean_squared_error(y_real_original, y_pred_original)
    rmse = np.sqrt(mse)
    mape = np.mean(np.abs((y_real_original - y_pred_original) / y_real_original)) * 100
    
    # Crear figura con mÃºltiples subgrÃ¡ficos
    fig = plt.figure(figsize=(16, 12))
    
    # 1. GrÃ¡fico general de predicciones
    plt.subplot(3, 2, 1)
    plt.plot(y_real_original, label='Valores Reales', color='blue', alpha=0.8)
    plt.plot(y_pred_original, label='Predicciones', color='red', alpha=0.8)
    plt.title('Predicciones vs Valores Reales - Vista General', fontsize=14, fontweight='bold')
    plt.xlabel('DÃ­as')
    plt.ylabel('Precio (USD)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Agregar mÃ©tricas en el grÃ¡fico
    metrics_text = f'MAE: ${mae:.2f}\nRMSE: ${rmse:.2f}\nMAPE: {mape:.2f}%'
    plt.text(0.02, 0.98, metrics_text, transform=plt.gca().transAxes, 
             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.8))
    
    # 2. Vista detallada (Ãºltimos n_dias)
    plt.subplot(3, 2, 2)
    dias_detalle = min(n_dias, len(y_real_original))
    plt.plot(y_real_original[-dias_detalle:], label='Valores Reales', 
             color='blue', marker='o', markersize=3, alpha=0.8)
    plt.plot(y_pred_original[-dias_detalle:], label='Predicciones', 
             color='red', marker='s', markersize=3, alpha=0.8)
    plt.title(f'Vista Detallada - Ãšltimos {dias_detalle} dÃ­as', fontsize=14, fontweight='bold')
    plt.xlabel('DÃ­as')
    plt.ylabel('Precio (USD)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # 3. GrÃ¡fico de dispersiÃ³n (Real vs Predicho)
    plt.subplot(3, 2, 3)
    plt.scatter(y_real_original, y_pred_original, alpha=0.6, color='purple')
    
    # LÃ­nea de referencia perfecta (y=x)
    min_val = min(y_real_original.min(), y_pred_original.min())
    max_val = max(y_real_original.max(), y_pred_original.max())
    plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='PredicciÃ³n Perfecta')
    
    plt.title('CorrelaciÃ³n: Real vs Predicho', fontsize=14, fontweight='bold')
    plt.xlabel('Precio Real (USD)')
    plt.ylabel('Precio Predicho (USD)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Calcular RÂ²
    correlation = np.corrcoef(y_real_original, y_pred_original)[0,1]
    r_squared = correlation ** 2
    plt.text(0.05, 0.95, f'RÂ² = {r_squared:.4f}', transform=plt.gca().transAxes,
             bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))
    
    # 4. Histograma de errores
    plt.subplot(3, 2, 4)
    errores = y_real_original - y_pred_original
    plt.hist(errores, bins=30, alpha=0.7, color='orange', edgecolor='black')
    plt.title('DistribuciÃ³n de Errores', fontsize=14, fontweight='bold')
    plt.xlabel('Error (Real - Predicho)')
    plt.ylabel('Frecuencia')
    plt.grid(True, alpha=0.3)
    
    # LÃ­nea vertical en error = 0
    plt.axvline(x=0, color='red', linestyle='--', label='Error = 0')
    plt.legend()
    
    # 5. Errores absolutos a lo largo del tiempo
    plt.subplot(3, 2, 5)
    errores_abs = np.abs(errores)
    plt.plot(errores_abs, color='orange', alpha=0.8)
    plt.title('Errores Absolutos a lo Largo del Tiempo', fontsize=14, fontweight='bold')
    plt.xlabel('DÃ­as')
    plt.ylabel('Error Absoluto (USD)')
    plt.grid(True, alpha=0.3)
    
    # Media mÃ³vil de errores
    window = 10
    if len(errores_abs) >= window:
        media_movil = np.convolve(errores_abs, np.ones(window)/window, mode='valid')
        plt.plot(range(window-1, len(errores_abs)), media_movil, 
                color='red', linewidth=2, label=f'Media mÃ³vil ({window} dÃ­as)')
        plt.legend()
    
    # 6. Resumen de mÃ©tricas
    plt.subplot(3, 2, 6)
    plt.axis('off')
    
    # Crear tabla de mÃ©tricas
    metricas_tabla = [
        ['MÃ©trica', 'Valor'],
        ['MAE (Error Absoluto Medio)', f'${mae:.2f}'],
        ['MSE (Error CuadrÃ¡tico Medio)', f'${mse:.2f}'],
        ['RMSE (RaÃ­z del MSE)', f'${rmse:.2f}'],
        ['MAPE (Error Porcentual)', f'{mape:.2f}%'],
        ['CorrelaciÃ³n', f'{correlation:.4f}'],
        ['RÂ² (Coef. DeterminaciÃ³n)', f'{r_squared:.4f}'],
        ['Datos de Prueba', f'{len(y_test)} puntos']
    ]
    
    # Mostrar tabla
    tabla = plt.table(cellText=metricas_tabla[1:], colLabels=metricas_tabla[0],
                     cellLoc='left', loc='center', bbox=[0, 0, 1, 1])
    tabla.auto_set_font_size(False)
    tabla.set_fontsize(10)
    tabla.scale(1, 2)
    
    # Colorear encabezados
    for i in range(len(metricas_tabla[0])):
        tabla[(0, i)].set_facecolor('#4CAF50')
        tabla[(0, i)].set_text_props(weight='bold', color='white')
    
    plt.title('Resumen de MÃ©tricas de EvaluaciÃ³n', fontsize=14, fontweight='bold', pad=20)
    
    plt.tight_layout()
    plt.show()
    
    print("âœ… Visualizaciones detalladas completadas")
    
    return {
        'mae': mae,
        'mse': mse,
        'rmse': rmse,
        'mape': mape,
        'correlation': correlation,
        'r_squared': r_squared
    }

# Visualizar ejemplos de secuencias
def visualizar_secuencias_ejemplo(X, y, scaler, n_ejemplos=3):
    """
    Visualiza ejemplos de secuencias de entrada y sus objetivos
    
    Args:
        X: Secuencias de entrada (shape: samples, timesteps, features)
        y: Objetivos correspondientes
        scaler: Scaler para desnormalizar
        n_ejemplos: NÃºmero de ejemplos a mostrar
    """
    print(f"\nğŸ” Visualizando {n_ejemplos} ejemplos de secuencias temporales...")
    
    plt.figure(figsize=(15, 5 * n_ejemplos))
    
    for i in range(min(n_ejemplos, len(X))):
        plt.subplot(n_ejemplos, 1, i+1)
        
        # Obtener secuencia y objetivo
        secuencia = X[i].flatten()  # (20 dÃ­as)
        objetivo = y[i]  # (1 dÃ­a siguiente)
        
        # Desnormalizar
        secuencia_original = scaler.inverse_transform(secuencia.reshape(-1, 1)).flatten()
        objetivo_original = scaler.inverse_transform([[objetivo]])[0][0]
        
        # Graficar secuencia
        dias_secuencia = range(len(secuencia_original))
        plt.plot(dias_secuencia, secuencia_original, 
                'b-o', label='Secuencia de entrada (20 dÃ­as)', markersize=4)
        
        # Graficar objetivo
        plt.plot([len(secuencia_original)], [objetivo_original], 
                'ro', markersize=8, label='Objetivo (dÃ­a siguiente)')
        
        plt.title(f'Ejemplo {i+1}: Secuencia â†’ PredicciÃ³n', fontsize=12, fontweight='bold')
        plt.xlabel('DÃ­as')
        plt.ylabel('Precio (USD)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # Conectar Ãºltimo punto de secuencia con objetivo
        plt.plot([len(secuencia_original)-1, len(secuencia_original)], 
                [secuencia_original[-1], objetivo_original], 'r--', alpha=0.5)
    
    plt.tight_layout()
    plt.show()
    
    print("âœ… VisualizaciÃ³n de secuencias completada")

# Ejecutar visualizaciones adicionales
print("\n" + "="*70)
print("ğŸ¯ EJECUTANDO VISUALIZACIONES ADICIONALES")
print("="*70)

# Visualizar historial de entrenamiento detallado
if 'historial_modelo' in locals():
    print("\nğŸ“ˆ Visualizando historial de entrenamiento detallado...")
    plot_training_history(historial_modelo)

# Visualizaciones detalladas de predicciones
print("\nğŸ” Generando anÃ¡lisis detallado de predicciones...")
metricas_detalladas = visualizar_predicciones_detalladas(
    modelo_lstm, X_test, y_test, scaler, n_dias=50
)

# Visualizar ejemplos de secuencias
print("\nğŸ‘ï¸ Mostrando ejemplos de secuencias temporales...")
visualizar_secuencias_ejemplo(X_test, y_test, scaler, n_ejemplos=3)

print(f"\nğŸ¯ AnÃ¡lisis completo finalizado!")
print(f"ğŸ“Š MÃ©tricas principales:")
for metrica, valor in metricas_detalladas.items():
    if metrica in ['mae', 'rmse']:
        print(f"   â€¢ {metrica.upper()}: ${valor:.2f}")
    elif metrica == 'mape':
        print(f"   â€¢ {metrica.upper()}: {valor:.2f}%")
    else:
        print(f"   â€¢ {metrica.upper()}: {valor:.4f}")

print(f"\nâœ… Modelo LSTM completamente implementado y evaluado!")
print(f"ğŸ”„ PrÃ³ximo paso: OptimizaciÃ³n con Algoritmos GenÃ©ticos")

# =============================================================================
# PREDICCIÃ“N FINAL: Â¿QUÃ‰ PREDICE EL MODELO PARA YPF?
# =============================================================================

def hacer_prediccion_final(modelo, datos_recientes, scaler, lookback_window=20):
    """
    Hace una predicciÃ³n especÃ­fica para el prÃ³ximo dÃ­a de la acciÃ³n YPF
    usando los datos mÃ¡s recientes disponibles
    
    Args:
        modelo: Modelo LSTM entrenado
        datos_recientes: Ãšltimos datos de la serie temporal
        scaler: Objeto MinMaxScaler para desnormalizar
        lookback_window: Ventana temporal del modelo
    
    Returns:
        dict: PredicciÃ³n detallada con contexto
    """
    print("\n" + "="*70)
    print("ğŸ¯ PREDICCIÃ“N FINAL: Â¿QUÃ‰ PREDICE EL MODELO PARA YPF?")
    print("="*70)
    
    # Obtener los Ãºltimos datos disponibles
    ultimos_datos = datos_recientes.tail(lookback_window)
    precio_actual = float(ultimos_datos.iloc[-1])
    fecha_actual = ultimos_datos.index[-1].date()
    
    print(f"\nğŸ“Š CONTEXTO ACTUAL:")
    print(f"   â€¢ Fecha del Ãºltimo precio: {fecha_actual}")
    print(f"   â€¢ Precio actual de YPF: ${precio_actual:.2f}")
    print(f"   â€¢ Ventana de anÃ¡lisis: {lookback_window} dÃ­as anteriores")
    
    # Normalizar los Ãºltimos datos
    ultimos_normalizados = scaler.transform(ultimos_datos.values.reshape(-1, 1))
    
    # Preparar entrada para el modelo
    X_prediccion = ultimos_normalizados.reshape(1, lookback_window, 1)
    
    # Hacer predicciÃ³n
    prediccion_normalizada = modelo.predict(X_prediccion, verbose=0)
    
    # Desnormalizar predicciÃ³n
    precio_predicho = float(scaler.inverse_transform(prediccion_normalizada)[0][0])
    
    # Calcular variaciones
    variacion_absoluta = precio_predicho - precio_actual
    variacion_porcentual = (variacion_absoluta / precio_actual) * 100
    
    # Determinar tendencia
    if variacion_porcentual > 2:
        tendencia = "ğŸ“ˆ FUERTE ALZA"
        interpretacion = "El modelo predice un aumento significativo"
    elif variacion_porcentual > 0.5:
        tendencia = "â†—ï¸ ALZA MODERADA"
        interpretacion = "El modelo predice un aumento moderado"
    elif variacion_porcentual > -0.5:
        tendencia = "â¡ï¸ ESTABLE"
        interpretacion = "El modelo predice estabilidad de precios"
    elif variacion_porcentual > -2:
        tendencia = "â†˜ï¸ BAJA MODERADA"
        interpretacion = "El modelo predice una disminuciÃ³n moderada"
    else:
        tendencia = "ğŸ“‰ FUERTE BAJA"
        interpretacion = "El modelo predice una disminuciÃ³n significativa"
    
    print(f"\nğŸ”® PREDICCIÃ“N PARA EL PRÃ“XIMO DÃA DE TRADING:")
    print(f"   â€¢ Precio predicho: ${precio_predicho:.2f}")
    print(f"   â€¢ VariaciÃ³n absoluta: ${variacion_absoluta:+.2f}")
    print(f"   â€¢ VariaciÃ³n porcentual: {variacion_porcentual:+.2f}%")
    print(f"   â€¢ Tendencia: {tendencia}")
    print(f"   â€¢ InterpretaciÃ³n: {interpretacion}")
    
    # AnÃ¡lisis de confianza basado en el rendimiento del modelo
    if 'metricas_detalladas' in globals():
        mape = metricas_detalladas.get('mape', 0)
        r_squared = metricas_detalladas.get('r_squared', 0)
        
        if mape < 5 and r_squared > 0.7:
            confianza = "ALTA"
            confianza_emoji = "ğŸŸ¢"
        elif mape < 10 and r_squared > 0.5:
            confianza = "MEDIA"
            confianza_emoji = "ğŸŸ¡"
        else:
            confianza = "BAJA"
            confianza_emoji = "ğŸ”´"
        
        print(f"\nğŸ¯ NIVEL DE CONFIANZA: {confianza_emoji} {confianza}")
        print(f"   â€¢ MAPE del modelo: {mape:.2f}%")
        print(f"   â€¢ RÂ² del modelo: {r_squared:.4f}")
    
    # Mostrar contexto histÃ³rico reciente
    print(f"\nğŸ“ˆ CONTEXTO DE LOS ÃšLTIMOS 5 DÃAS:")
    ultimos_5_dias = ultimos_datos.tail(5)
    for i in range(len(ultimos_5_dias)):
        fecha = ultimos_5_dias.index[i]
        precio = ultimos_5_dias.iloc[i]
        es_ultimo = i == len(ultimos_5_dias) - 1
        emoji = "ğŸ‘‰" if es_ultimo else "  "
        # Manejar diferentes tipos de fecha
        if hasattr(fecha, 'date'):
            fecha_str = fecha.date()
        else:
            fecha_str = str(fecha)
        print(f"   {emoji} {fecha_str}: ${float(precio):.2f}")
    
    return {
        'precio_actual': precio_actual,
        'precio_predicho': precio_predicho,
        'variacion_absoluta': variacion_absoluta,
        'variacion_porcentual': variacion_porcentual,
        'tendencia': tendencia,
        'interpretacion': interpretacion,
        'fecha_prediccion': fecha_actual
    }

def mostrar_escenarios_prediccion(prediccion_base, precio_actual):
    """
    Muestra diferentes escenarios basados en la predicciÃ³n
    """
    print(f"\nğŸ“‹ ESCENARIOS DE INVERSIÃ“N:")
    
    precio_predicho = prediccion_base['precio_predicho']
    variacion_pct = prediccion_base['variacion_porcentual']
    
    # Escenarios de inversiÃ³n
    inversion_ejemplo = 10000  # $10,000 como ejemplo
    acciones_posibles = inversion_ejemplo / precio_actual
    
    print(f"\nğŸ’° Ejemplo con inversiÃ³n de ${inversion_ejemplo:,.0f}:")
    print(f"   â€¢ Acciones que se pueden comprar: {acciones_posibles:.0f}")
    
    if variacion_pct > 0:
        ganancia_potencial = acciones_posibles * prediccion_base['variacion_absoluta']
        print(f"   â€¢ Ganancia potencial: ${ganancia_potencial:+.2f}")
        print(f"   â€¢ ROI esperado: {variacion_pct:+.2f}%")
        print(f"   â€¢ ğŸ“ˆ Estrategia sugerida: COMPRAR")
    else:
        perdida_potencial = acciones_posibles * prediccion_base['variacion_absoluta']
        print(f"   â€¢ PÃ©rdida potencial: ${perdida_potencial:+.2f}")
        print(f"   â€¢ ROI esperado: {variacion_pct:+.2f}%")
        print(f"   â€¢ ğŸ“‰ Estrategia sugerida: MANTENER o VENDER")
    
    print(f"\nâš ï¸ DISCLAIMER:")
    print(f"   â€¢ Esta es una predicciÃ³n basada en datos histÃ³ricos")
    print(f"   â€¢ Los mercados financieros son impredecibles")
    print(f"   â€¢ Siempre consulte con un asesor financiero")
    print(f"   â€¢ No invierta mÃ¡s de lo que puede permitirse perder")

def visualizar_prediccion_final(datos_historicos, prediccion, scaler):
    """
    Crea una visualizaciÃ³n especÃ­fica de la predicciÃ³n final
    """
    print(f"\nğŸ“Š Creando visualizaciÃ³n de la predicciÃ³n final...")
    
    # Preparar datos para visualizaciÃ³n
    ultimos_30_dias = datos_historicos.tail(30)
    fechas = ultimos_30_dias.index
    precios = ultimos_30_dias.values
    
    # Crear fecha para la predicciÃ³n (prÃ³ximo dÃ­a de trading)
    from datetime import timedelta
    fecha_prediccion = fechas[-1] + timedelta(days=1)
    precio_predicho = prediccion['precio_predicho']
    precio_actual = float(precios[-1])  # Convertir a escalar
    
    plt.figure(figsize=(14, 8))
    
    # GrÃ¡fico principal
    plt.plot(fechas, precios, 'b-', linewidth=2, label='Precios HistÃ³ricos', marker='o', markersize=4)
    
    # Punto actual
    plt.plot(fechas[-1], precio_actual, 'go', markersize=10, label=f'Precio Actual: ${precio_actual:.2f}')
    
    # PredicciÃ³n
    plt.plot(fecha_prediccion, precio_predicho, 'rs', markersize=12, 
             label=f'PredicciÃ³n: ${precio_predicho:.2f}')
    
    # LÃ­nea conectora
    plt.plot([fechas[-1], fecha_prediccion], [precio_actual, precio_predicho], 
             'r--', alpha=0.7, linewidth=2)
    
    # Formateo
    plt.title('YPF - PredicciÃ³n para el PrÃ³ximo DÃ­a de Trading', fontsize=16, fontweight='bold')
    plt.xlabel('Fecha', fontsize=12)
    plt.ylabel('Precio (USD)', fontsize=12)
    plt.legend(fontsize=11)
    plt.grid(True, alpha=0.3)
    plt.xticks(rotation=45)
    
    # AÃ±adir informaciÃ³n de la predicciÃ³n
    variacion = prediccion['variacion_porcentual']
    color_texto = 'green' if variacion > 0 else 'red'
    plt.text(0.02, 0.98, 
             f"VariaciÃ³n Predicha: {variacion:+.2f}%\n{prediccion['tendencia']}", 
             transform=plt.gca().transAxes, fontsize=12, fontweight='bold',
             verticalalignment='top', color=color_texto,
             bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))
    
    plt.tight_layout()
    plt.show()
    
    print("âœ… VisualizaciÃ³n de predicciÃ³n final completada")

# EJECUTAR PREDICCIÃ“N FINAL
print("\n" + "="*70)
print("ğŸš€ EJECUTANDO PREDICCIÃ“N FINAL PARA YPF")
print("="*70)

# Hacer predicciÃ³n para el prÃ³ximo dÃ­a
prediccion_final = hacer_prediccion_final(modelo_lstm, datos_ypf, scaler, LOOKBACK_WINDOW)

# Mostrar escenarios de inversiÃ³n
mostrar_escenarios_prediccion(prediccion_final, prediccion_final['precio_actual'])

# Visualizar predicciÃ³n
visualizar_prediccion_final(datos_ypf, prediccion_final, scaler)

# RESUMEN EJECUTIVO FINAL
print("\n" + "="*70)
print("ğŸ“‹ RESUMEN EJECUTIVO - PREDICCIÃ“N LSTM PARA YPF")
print("="*70)

print(f"""
ğŸ¯ PREDICCIÃ“N PRINCIPAL:
   El modelo LSTM predice que YPF cotizarÃ¡ a ${prediccion_final['precio_predicho']:.2f} 
   en el prÃ³ximo dÃ­a de trading, representando una variaciÃ³n de {prediccion_final['variacion_porcentual']:+.2f}%

ğŸ“Š FUNDAMENTO TÃ‰CNICO:
   â€¢ Modelo entrenado con {NUM_DATOS} puntos histÃ³ricos
   â€¢ Arquitectura: 2 capas LSTM con 50 unidades cada una
   â€¢ Ventana temporal: {LOOKBACK_WINDOW} dÃ­as de historia para predicciÃ³n
   â€¢ Accuracy del modelo: RÂ² = {metricas_detalladas.get('r_squared', 0):.4f}

ğŸ’¡ INTERPRETACIÃ“N:
   {prediccion_final['interpretacion']}

âš ï¸ RIESGOS Y LIMITACIONES:
   â€¢ Modelo basado en patrones histÃ³ricos
   â€¢ No considera eventos fundamentales o noticias
   â€¢ Mercados pueden ser impredecibles en el corto plazo
   â€¢ Error promedio del modelo: Â±{metricas_detalladas.get('mape', 0):.1f}%

ğŸ”® PRÃ“XIMOS PASOS:
   âœ… Modelo LSTM base implementado
   ğŸ”„ Pendiente: OptimizaciÃ³n con Algoritmos GenÃ©ticos
   ğŸ¯ Objetivo: Mejorar precisiÃ³n mediante evoluciÃ³n de hiperparÃ¡metros
""")

#print(f"\nğŸ‰ IMPLEMENTACIÃ“N LSTM COMPLETADA CON PREDICCIÃ“N CLARA!")
#print(f"ğŸš€ Sistema listo para la fase de optimizaciÃ³n con Algoritmos GenÃ©ticos")

