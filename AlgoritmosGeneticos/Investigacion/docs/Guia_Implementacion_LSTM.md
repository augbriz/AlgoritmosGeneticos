# **GuÃ­a de ImplementaciÃ³n LSTM para PredicciÃ³n de Acciones YPF**
## **Proyecto: Modelado del comportamiento caÃ³tico de YPF mediante Algoritmos GenÃ©ticos y Redes LSTM**

---

## **1. Â¿QUÃ‰ SON LAS REDES LSTM Y POR QUÃ‰ LAS NECESITAMOS?**

### **1.1 El Problema con los Precios de Acciones**

Imagina que quieres predecir si el precio de YPF va a subir o bajar maÃ±ana. Â¿QuÃ© informaciÃ³n usarÃ­as?

- El precio de hoy
- El precio de ayer  
- El precio de la semana pasada
- Â¿Y el precio de hace un mes? Â¿Sigue siendo relevante?

Los precios de las acciones tienen **memoria**: lo que pasÃ³ en el pasado influye en lo que va a pasar maÃ±ana. Pero hay un problema: **Â¿cuÃ¡nto del pasado debemos recordar?**

### **1.2 Â¿QuÃ© es una Red LSTM?**

**LSTM** significa "**Long Short-Term Memory**" (Memoria de Largo y Corto Plazo). Es un tipo especial de red neuronal que fue diseÃ±ada especÃ­ficamente para **recordar informaciÃ³n importante del pasado** y **olvidar informaciÃ³n irrelevante**.

Piensa en una LSTM como un **asistente muy inteligente** que:
- **Recuerda** los eventos importantes del pasado que pueden afectar el futuro
- **Olvida** los ruidos y fluctuaciones menores que no son importantes
- **Aprende** quÃ© informaciÃ³n del pasado es realmente Ãºtil para hacer predicciones

### **1.3 Â¿CÃ³mo Funciona una LSTM? (ExplicaciÃ³n Simple)**

Una red LSTM tiene tres "**puertas**" que funcionan como filtros inteligentes:

#### **ğŸšª Puerta del Olvido (Forget Gate)**
- **Â¿QuÃ© hace?** Decide quÃ© informaciÃ³n del pasado ya no es Ãºtil y la descarta
- **Ejemplo con YPF:** Si hubo una caÃ­da hace 6 meses por una noticia especÃ­fica que ya no es relevante, la puerta del olvido la elimina de la memoria

#### **ğŸšª Puerta de Entrada (Input Gate)** 
- **Â¿QuÃ© hace?** Decide quÃ© nueva informaciÃ³n del presente es importante y vale la pena recordar
- **Ejemplo con YPF:** Si hoy hay un anuncio importante sobre nuevas reservas de petrÃ³leo, esta puerta decide que es informaciÃ³n valiosa para recordar

#### **ğŸšª Puerta de Salida (Output Gate)**
- **Â¿QuÃ© hace?** Decide quÃ© informaciÃ³n usar para hacer la predicciÃ³n de maÃ±ana
- **Ejemplo con YPF:** Combina la informaciÃ³n relevante del pasado con la informaciÃ³n nueva para predecir el precio de maÃ±ana

### **1.4 Â¿Por QuÃ© LSTM es Perfecta para YPF?**

Las acciones de YPF tienen caracterÃ­sticas especiales que hacen que LSTM sea ideal:

**ğŸ”„ Patrones que se Repiten**
- YPF sigue ciertos patrones estacionales (ej: mayor demanda en invierno)
- LSTM puede aprender y recordar estos patrones

**ğŸ“ˆ Volatilidad Compleja**
- Los precios pueden tener subidas y bajadas bruscas
- LSTM puede distinguir entre movimientos importantes y ruido del mercado

**ğŸŒ Influencias MÃºltiples**
- Precio del petrÃ³leo internacional
- PolÃ­tica argentina
- Resultados trimestrales de la empresa
- LSTM puede recordar cÃ³mo estos factores afectaron en el pasado

### **1.5 Datos Necesarios: Â¿Son Suficientes 1000 Puntos?**

Para nuestro proyecto con YPF, **1000 datos histÃ³ricos (aproximadamente 4 aÃ±os) son SUFICIENTES** porque:

âœ… **Captura Ciclos Importantes:** 4 aÃ±os incluyen diferentes crisis, bonanzas y ciclos polÃ­ticos
âœ… **Evita Sobreajuste:** Con series caÃ³ticas como YPF, demasiados datos pueden incluir informaciÃ³n obsoleta
âœ… **DivisiÃ³n Adecuada:** Permite dividir los datos en entrenamiento (700), validaciÃ³n (150) y prueba (150)
âœ… **Comportamiento CaÃ³tico:** Para sistemas caÃ³ticos, la informaciÃ³n muy antigua pierde relevancia

---

## **2. GUÃA DE IMPLEMENTACIÃ“N LSTM**

### **2.1 Fase 1: PreparaciÃ³n de los Datos**

**ğŸ¯ Objetivo:** Convertir los precios histÃ³ricos de YPF en un formato que la LSTM pueda entender

**Â¿QuÃ© haremos?**
1. **Limpiar los datos:** Eliminar dÃ­as sin trading, datos faltantes, errores
2. **Normalizar:** Convertir todos los precios a una escala de 0 a 1 para facilitar el aprendizaje
3. **Crear secuencias:** En lugar de datos individuales, crear "ventanas" de tiempo
   - Ejemplo: "Los Ãºltimos 20 dÃ­as de precios" â†’ "Precio de maÃ±ana"
4. **Dividir el dataset:** Separar en entrenamiento, validaciÃ³n y prueba

**Â¿Por quÃ© estas ventanas?**
- Si usamos los Ãºltimos 20 dÃ­as para predecir el dÃ­a 21, la LSTM aprende patrones como:
  - "Cuando hay 3 dÃ­as consecutivos de subida, suele haber una correcciÃ³n"
  - "Los lunes despuÃ©s de bajas fuertes del viernes suelen recuperarse"

### **2.2 Fase 2: ConstrucciÃ³n del Modelo LSTM**

**ğŸ¯ Objetivo:** Crear la arquitectura de la red neuronal

**Componentes principales:**

#### **ğŸ“Š Capa de Entrada**
- Recibe las secuencias de precios (ej: Ãºltimos 20 dÃ­as)
- Es como mostrarle a la red el "contexto" para hacer la predicciÃ³n

#### **ğŸ§  Capas LSTM**
- El "cerebro" que procesa la informaciÃ³n temporal
- Pueden ser una o varias capas apiladas
- MÃ¡s capas = mayor capacidad de aprender patrones complejos (pero tambiÃ©n mÃ¡s riesgo de sobreajuste)

#### **ğŸ¯ Capa de Salida**
- Produce la predicciÃ³n final: el precio estimado para maÃ±ana
- Es un solo nÃºmero (el precio predicho)

**Â¿CÃ³mo decide cuÃ¡ntas capas usar?**
Esto lo optimizaremos despuÃ©s con algoritmos genÃ©ticos, pero la idea general es:
- **1 capa LSTM:** Para patrones simples
- **2-3 capas LSTM:** Para patrones mÃ¡s complejos (nuestro caso probable)
- **MÃ¡s de 3 capas:** Raramente necesario, puede causar sobreentrenamiento

### **2.3 Fase 3: Entrenamiento del Modelo**

**ğŸ¯ Objetivo:** EnseÃ±ar a la LSTM a reconocer patrones en los datos de YPF

**Â¿CÃ³mo aprende la LSTM?**
1. **PredicciÃ³n:** La red ve los primeros 20 dÃ­as y predice el dÃ­a 21
2. **ComparaciÃ³n:** Compara su predicciÃ³n con el precio real del dÃ­a 21
3. **Ajuste:** Si se equivocÃ³, ajusta sus parÃ¡metros internos para mejorar
4. **RepeticiÃ³n:** Hace esto miles de veces con diferentes secuencias

**Controles importantes:**
- **Early Stopping:** Si la red deja de mejorar, para el entrenamiento automÃ¡ticamente
- **ValidaciÃ³n:** Usa datos que nunca vio para verificar que no estÃ¡ "memorizando"

---

## **3. OPTIMIZACIÃ“N CON ALGORITMOS GENÃ‰TICOS**

### **3.1 Â¿QuÃ© son los HiperparÃ¡metros?**

Los **hiperparÃ¡metros** son las "configuraciones" del modelo LSTM que nosotros tenemos que decidir antes del entrenamiento. Es como elegir las especificaciones de un auto antes de comprarlo.

**ğŸ”§ Principales HiperparÃ¡metros para LSTM:**

#### **ğŸ—ï¸ Arquitectura del Modelo**
- **NÃºmero de capas LSTM:** Â¿1, 2 o 3 capas?
- **NÃºmero de neuronas por capa:** Â¿32, 64, 128 neuronas?
- **Ventana temporal:** Â¿Usar 10, 20 o 30 dÃ­as pasados para predecir?

#### **ğŸ“ ParÃ¡metros de Aprendizaje**
- **Tasa de aprendizaje:** Â¿QuÃ© tan rÃ¡pido aprende? (muy rÃ¡pido puede ser inestable, muy lento puede ser ineficiente)
- **TamaÃ±o de lote (batch size):** Â¿CuÃ¡ntos ejemplos ve antes de ajustar?
- **Ã‰pocas:** Â¿CuÃ¡ntas veces repasa todos los datos?

#### **ğŸ›¡ï¸ RegularizaciÃ³n**
- **Dropout:** Â¿QuÃ© porcentaje de neuronas "apagar" para evitar sobreajuste?
- **RegularizaciÃ³n L2:** Â¿CuÃ¡nto penalizar la complejidad del modelo?

### **3.2 El Problema de la OptimizaciÃ³n**

Imagina que tienes que encontrar la **mejor combinaciÃ³n** de todos estos parÃ¡metros:
- 3 opciones de capas Ã— 4 opciones de neuronas Ã— 5 opciones de ventana temporal Ã— 4 opciones de tasa de aprendizaje = **240 combinaciones diferentes**

Y esto es solo con unos pocos parÃ¡metros. En realidad, hay **miles de combinaciones posibles**.

**Â¿CÃ³mo encontrar la mejor?**
- **MÃ©todo manual:** Probar uno por uno â†’ TomarÃ­a meses
- **BÃºsqueda en grilla:** Probar todas las combinaciones â†’ TomarÃ­a semanas
- **Algoritmos GenÃ©ticos:** EvoluciÃ³n inteligente â†’ DÃ­as

### **3.3 Â¿CÃ³mo Funcionan los Algoritmos GenÃ©ticos para OptimizaciÃ³n?**

Los **Algoritmos GenÃ©ticos** imitan la evoluciÃ³n natural para encontrar las mejores configuraciones:

#### **ğŸ§¬ Paso 1: PoblaciÃ³n Inicial**
- Crear 20-50 configuraciones LSTM diferentes (aleatorias)
- Cada configuraciÃ³n es un "individuo" con su propio "ADN" (hiperparÃ¡metros)

#### **ğŸƒ Paso 2: EvaluaciÃ³n (Fitness)**
- Entrenar cada configuraciÃ³n LSTM con los datos de YPF
- Medir quÃ© tan bien predice (ej: % de aciertos direccionales)
- Asignar una "puntuaciÃ³n de fitness" a cada configuraciÃ³n

#### **â¤ï¸ Paso 3: SelecciÃ³n**
- Las configuraciones que predicen mejor tienen mÃ¡s probabilidad de "reproducirse"
- Las configuraciones malas tienen menos probabilidad de continuar

#### **ğŸ§¬ Paso 4: Cruce (Crossover)**
- Combinar dos configuraciones "padres" exitosas para crear "hijos"
- Ejemplo: Padre A (2 capas, 64 neuronas) + Padre B (3 capas, 32 neuronas) = Hijo (2 capas, 32 neuronas)

#### **âš¡ Paso 5: MutaciÃ³n**
- Cambiar aleatoriamente algunos parÃ¡metros de los "hijos"
- Esto introduce variabilidad y puede descubrir mejores configuraciones

#### **ğŸ”„ Paso 6: Nueva GeneraciÃ³n**
- Repetir el proceso con la nueva poblaciÃ³n
- DespuÃ©s de 20-50 generaciones, converge hacia la mejor configuraciÃ³n

### **3.4 Â¿Por QuÃ© Funcionan Mejor los AG para LSTM?**

**ğŸ¯ ExploraciÃ³n Inteligente**
- No prueban todas las combinaciones, sino que se enfocan en las mÃ¡s prometedoras
- Balancean exploraciÃ³n (probar cosas nuevas) vs. explotaciÃ³n (mejorar lo bueno)

**ğŸ”„ Adaptabilidad**
- Si una estrategia funciona, la enfatizan mÃ¡s
- Si una estrategia falla, la abandonan rÃ¡pidamente

**ğŸª Diversidad**
- Mantienen mÃºltiples enfoques simultÃ¡neamente
- Evitan quedarse atascados en Ã³ptimos locales

---

## **4. MÃ‰TRICAS DE EVALUACIÃ“N**

### **4.1 Â¿CÃ³mo Sabemos si Nuestro Modelo es Bueno?**

Con las acciones, no solo importa **quÃ© tan cerca estamos del precio real**, sino tambiÃ©n **si acertamos la direcciÃ³n** (Â¿sube o baja?).

### **4.2 MÃ©tricas Principales**

#### **ğŸ“Š MÃ©tricas de PrecisiÃ³n NumÃ©rica**

**ğŸ¯ Error CuadrÃ¡tico Medio (MSE/RMSE)**
- **Â¿QuÃ© mide?** QuÃ© tan lejos estÃ¡n nuestras predicciones del precio real
- **Â¿Por quÃ© importa?** Si predecimos $15 y el precio real es $10, el error es grande
- **InterpretaciÃ³n:** MÃ¡s bajo = mejor

**ğŸ“ Error Absoluto Medio (MAE)**
- **Â¿QuÃ© mide?** El error promedio en tÃ©rminos absolutos
- **Â¿Por quÃ© importa?** MÃ¡s fÃ¡cil de interpretar (en pesos argentinos)
- **Ejemplo:** "En promedio, nos equivocamos por $2 por acciÃ³n"

#### **ğŸ§­ MÃ©tricas Direccionales (Las MÃ¡s Importantes para Trading)**

**ğŸ“ˆ PrecisiÃ³n Direccional**
- **Â¿QuÃ© mide?** Â¿En quÃ© porcentaje de casos acertamos si el precio sube o baja?
- **Â¿Por quÃ© es crucial?** Para trading, importa mÃ¡s la direcciÃ³n que el valor exacto
- **InterpretaciÃ³n:** "Acertamos la direcciÃ³n en el 65% de los casos"

**ğŸ’° Retorno de InversiÃ³n Simulado**
- **Â¿QuÃ© mide?** Si siguiÃ©ramos las predicciones del modelo, Â¿ganarÃ­amos dinero?
- **Â¿CÃ³mo funciona?** 
  - Si el modelo predice subida â†’ Compramos
  - Si el modelo predice bajada â†’ Vendemos o no compramos
- **InterpretaciÃ³n:** "Siguiendo el modelo, habrÃ­amos ganado 15% en 6 meses"

### **4.3 Â¿CÃ³mo Medimos la Eficiencia de los Algoritmos GenÃ©ticos?**

#### **ğŸ“ˆ EvoluciÃ³n del Fitness**
- **Â¿QuÃ© observamos?** CÃ³mo mejora la mejor configuraciÃ³n generaciÃ³n tras generaciÃ³n
- **Indicadores de Ã©xito:**
  - La puntuaciÃ³n del mejor individuo aumenta con el tiempo
  - La puntuaciÃ³n se estabiliza (converge) despuÃ©s de varias generaciones
  - No hay mejoras por muchas generaciones seguidas

#### **ğŸ¯ ComparaciÃ³n con MÃ©todos Tradicionales**
- **BÃºsqueda aleatoria:** Â¿Los AG encuentran mejores configuraciones que elegir al azar?
- **ConfiguraciÃ³n manual:** Â¿Los AG superan las configuraciones elegidas por expertos?
- **Tiempo de convergencia:** Â¿CuÃ¡ntas generaciones necesita para encontrar una buena soluciÃ³n?

#### **ğŸ” Diversidad de la PoblaciÃ³n**
- **Â¿QuÃ© medimos?** QuÃ© tan diferentes son las configuraciones en cada generaciÃ³n
- **Indicador saludable:** Mantener diversidad evita convergencia prematura
- **SeÃ±al de alerta:** Si todas las configuraciones se vuelven muy similares muy rÃ¡pido

### **4.4 ValidaciÃ³n Robusta**

#### **ğŸ“… ValidaciÃ³n Temporal**
- **Â¿Por quÃ© especial?** Con series temporales, NO podemos usar validaciÃ³n cruzada normal
- **Â¿CÃ³mo lo hacemos?** Entrenamos con datos del pasado, validamos con datos del futuro
- **Ejemplo:** Entrenar con 2020-2022, validar con 2023, probar con 2024

#### **ğŸ² MÃºltiples Semillas**
- **Â¿Por quÃ©?** Los algoritmos genÃ©ticos tienen componentes aleatorios
- **Â¿CÃ³mo?** Correr el mismo experimento 5-10 veces con diferentes semillas aleatorias
- **Â¿QuÃ© buscamos?** Resultados consistentes entre corridas

---

## **5. PLAN DE TRABAJO PASO A PASO**

### **1: Entender y Preparar los Datos**
- AnÃ¡lisis exploratorio de la serie de YPF
- Implementar la preparaciÃ³n de datos
- Crear las ventanas temporales
- Dividir en entrenamiento/validaciÃ³n/prueba

### **2: Construir LSTM BÃ¡sica**
- Implementar el modelo LSTM con configuraciÃ³n fija
- Entrenarlo y evaluar resultados base
- Entender cÃ³mo se comporta con datos de YPF

### **3: Implementar Algoritmos GenÃ©ticos**
- Definir el espacio de hiperparÃ¡metros
- Implementar el algoritmo genÃ©tico
- Correr la optimizaciÃ³n y analizar resultados

### **4: EvaluaciÃ³n Final**
- Probar el mejor modelo en datos de test
- Comparar con mÃ©todos benchmark
- Analizar resultados y documentar conclusiones


âœ… **QuÃ© son las LSTM** y por quÃ© son perfectas para predecir precios de YPF
âœ… **CÃ³mo funcionan** sin entrar en complicaciones matemÃ¡ticas  
âœ… **QuÃ© son los hiperparÃ¡metros** y por quÃ© necesitamos optimizarlos
âœ… **CÃ³mo los algoritmos genÃ©ticos** encuentran las mejores configuraciones automÃ¡ticamente
âœ… **CÃ³mo medir el Ã©xito** tanto del modelo como de la optimizaciÃ³n




